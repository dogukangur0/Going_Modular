{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP78PP6DQyl1NbKebN6D7cb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hzwLrnnbGs9V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "1sTQtL1vNjN2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = pathlib.Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exist.\")\n",
        "else:\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "  print(f\"{image_path} directory created.\")\n",
        "\n",
        "with open(data_path/\"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading pizza, steak, sushi data...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping pizza, steak, sushi data...\")\n",
        "  zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE9KPD5ENk6f",
        "outputId": "32693285-c469-4d1c-d159-12dab5c2d479"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory created.\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "def create_dataLoader(\n",
        "    train_dir,\n",
        "    test_dir,\n",
        "    transform,\n",
        "    batch_size,\n",
        "    num_workers : int = NUM_WORKERS\n",
        "):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "  \"\"\"\n",
        "  train_set = torchvision.datasets.ImageFolder(train_dir, transform)\n",
        "  test_set = torchvision.datasets.ImageFolder(test_dir, transform)\n",
        "\n",
        "  train_dataLoader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, num_workers = num_workers, pin_memory = True, shuffle = True)\n",
        "  test_dataLoader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, num_workers = num_workers, pin_memory = True, shuffle = False)\n",
        "\n",
        "  classes = train_set.classes\n",
        "\n",
        "  return train_dataLoader, test_dataLoader, classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aME17LEGNp0I",
        "outputId": "2097e0c7-df07-4200-c086-f85f3539d1ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_build.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape, hidden_units, output_shape):\n",
        "    super().__init__()\n",
        "    self.conv_block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.conv_block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = hidden_units*13*13, out_features = output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.classifier(self.conv_block2(self.conv_block1(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6uTl9IKOVpx",
        "outputId": "8b1d8623-fbfb-4a34-950d-db6a55222c85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_build.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train_test.py\n",
        "import torch\n",
        "\n",
        "def train_step(model, trainDataLoader, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "  total_train_acc = 0\n",
        "  for train_images, train_labels in trainDataLoader:\n",
        "    optimizer.zero_grad()\n",
        "    y_train_logits = model(train_images)\n",
        "    train_loss = loss_fn(y_train_logits, train_labels)\n",
        "    total_train_loss += train_loss.item()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    y_train_pred = torch.argmax(y_train_logits, dim=1)\n",
        "    total_train_acc += ((y_train_pred == train_labels).sum().item() / len(train_labels))\n",
        "  total_train_loss = total_train_loss / len(trainDataLoader)\n",
        "  total_train_acc = total_train_acc / len(trainDataLoader)\n",
        "\n",
        "  return total_train_loss, total_train_acc\n",
        "\n",
        "def test_step(model, testDataLoader, loss_fn):\n",
        "  model.eval()\n",
        "  total_test_loss = 0\n",
        "  total_test_acc = 0\n",
        "  with torch.inference_mode():\n",
        "    for test_images, test_labels in testDataLoader:\n",
        "      y_test_logits = model(test_images)\n",
        "      test_loss = loss_fn(y_test_logits, test_labels)\n",
        "      total_test_loss += test_loss.item()\n",
        "      y_test_pred = torch.argmax(y_test_logits, dim = 1)\n",
        "      total_test_acc += ((y_test_pred == test_labels).sum().item() / len(test_labels))\n",
        "    total_test_loss = total_test_loss / len(testDataLoader)\n",
        "    total_test_acc = total_test_acc / len(testDataLoader)\n",
        "\n",
        "  return total_test_loss, total_test_acc\n",
        "\n",
        "def train_model(model, trainDataLoader, testDataLoder, optimizer, loss_fn, epochs):\n",
        "  results = {\n",
        "      \"train_loss\" : [],\n",
        "      \"train_acc\" : [],\n",
        "      \"test_loss\" : [],\n",
        "      \"test_acc\" : []\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_step(model, trainDataLoader, optimizer, loss_fn)\n",
        "    test_loss, test_acc = test_step(model, testDataLoder, loss_fn)\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7al-QLCOdGm",
        "outputId": "3733f0b9-d2a6-4ad5-e4bb-e4ba4622df4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train_test.py\n"
          ]
        }
      ]
    }
  ]
}